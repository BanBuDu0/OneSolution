{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import  svm, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(path_train,path_test):\n",
    "    file_train = pd.read_csv(path_train,encoding=\"utf-8\", skipinitialspace=True)\n",
    "    file_test = pd.read_csv(path_test, encoding=\"utf-8\", skipinitialspace=True)\n",
    "    df_test = pd.DataFrame(file_test)\n",
    "    df_train = pd.DataFrame(file_train)\n",
    "\n",
    "    df_train_kmeans = df_train.iloc[5071:, :-1]  # 去除flag列,获得kmeans部分。\n",
    "    df_train = df_train.iloc[:5071,:] # 获得有标记部分\n",
    "\n",
    "    df_train_kmeans = df_train_kmeans.dropna()\n",
    "    df_train = df_train.dropna()\n",
    "    df_test = df_test.dropna()\n",
    "    # 删除整行缺失值\n",
    "\n",
    "    #　这里变成矩阵了\n",
    "    X_train, Y_train = np.split(np.array(df_train), (-1,), axis=1)\n",
    "    X_test, Y_test = np.split(np.array(df_test), (-1,), axis=1)\n",
    "    Y_train = Y_train.astype('int')\n",
    "    Y_test =Y_test.astype('int')\n",
    "    df_train_kmeans = np.array(df_train_kmeans)\n",
    "\n",
    "\n",
    "\n",
    "    # 这里是按照下面方差筛选后的特征值,用于和下面onehot合并\n",
    "    trainx = X_train[:,19:]\n",
    "    testx = X_test[:,19:]\n",
    "    train_kmeans = df_train_kmeans[:, 19:]\n",
    "\n",
    "    trainx = Normalizer().fit_transform(trainx)\n",
    "    testx = Normalizer().fit_transform(testx)\n",
    "    train_kmeans = Normalizer().fit_transform(train_kmeans)\n",
    "\n",
    "    # # 标准化和归一化不知道搞那个，干脆都写了，好像一般是归一化\n",
    "    # Scaler_X = preprocessing.MinMaxScaler()\n",
    "    # VarianceThreshold(threshold=3).fit_transform(iris.data)\n",
    "    # X_train_scaler = Scaler_X.fit_transform(X_train)\n",
    "    # X_test_scaler = Scaler_X.transform(X_test)\n",
    "    #\n",
    "    le = LabelEncoder()\n",
    "    enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "    #对每一列中文进行赋值，这里有个问题，就是前面流程搞完后后就变矩阵了，无法按列名字搜索,所以注意下读入的中文数据是从第几列开始的。这是可改进点\n",
    "    i=2\n",
    "    while(i<6):\n",
    "\n",
    "        le.fit(X_train[:,i].reshape(-1, 1))\n",
    "        X_train[:,i] = le.transform(X_train[:,i].reshape(-1, 1))\n",
    "        X_test[:,i] = le.transform(X_test[:,i].reshape(-1, 1))\n",
    "        df_train_kmeans[:,i] = le.transform(df_train_kmeans[:,i].reshape(-1, 1))\n",
    "\n",
    "        enc.fit(X_train[:,i].reshape(-1, 1))\n",
    "        kkk1 = enc.transform(X_train[:,i].reshape(-1, 1))\n",
    "        kkk2 = enc.transform(X_test[:,i].reshape(-1, 1))\n",
    "        kkk3 = enc.transform(df_train_kmeans[:,i].reshape(-1, 1))\n",
    "        kkk1 = sparse.lil_matrix(kkk1)\n",
    "        kkk2 = sparse.lil_matrix(kkk2)\n",
    "        kkk3 = sparse.lil_matrix(kkk3)\n",
    "        trainx = sparse.hstack((trainx, kkk1))\n",
    "        testx = sparse.hstack((testx,kkk2))\n",
    "        train_kmeans = sparse.hstack((train_kmeans,kkk3))\n",
    "        i +=1\n",
    "\n",
    "    trainx = trainx.toarray()\n",
    "    testx = testx.toarray()\n",
    "    train_kmeans = train_kmeans.toarray()\n",
    "    data_train = PCA(n_components=2).fit_transform(trainx)\n",
    "    data_test = PCA(n_components=2).fit_transform(testx)\n",
    "    df_train_kmeans = PCA(n_components=2).fit_transform(train_kmeans)\n",
    "\n",
    "    # 用来计算哪几项没区分度，算出来后发现就最后几项特征值有区分度，就在前面直接改了\n",
    "    # data_train = VarianceThreshold(threshold=0.005).fit_transform(trainx)\n",
    "\n",
    "    # data_test = VarianceThreshold(threshold=0.005).fit_transform(testx)\n",
    "    return  data_train,data_test,Y_train,Y_test,df_train_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"./HuiZong_Train.csv\"\n",
    "path_test = \"./HuiZong_test.csv\"\n",
    "data_train, data_test, Y_train, Y_test , train_kmeans = analyse(path_train, path_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
